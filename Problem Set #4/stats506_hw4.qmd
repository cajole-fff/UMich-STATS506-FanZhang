---
title: "STATS506 Problem Set #04"
author: "Fan Zhang"
date: "10/19/2023"
format:
    html:
        embed-resources: true 
        code-line-numbers: true
        # code-fold: true
editor: 
    render-on-save: false 
---

GitHub Repository: [https://github.com/cajole-fff/UMich-STATS506-FanZhang/](https://github.com/cajole-fff/UMich-STATS506-FanZhang/)

## Problem 1 - Tidyverse 

Use the `tidyverse` for this problem. In particular, use piping and `dplyr` as much as you are able. **Note:** Use of any deprecated functions will result in a point loss. 

Install and load the package `nycflights13`.


```{r} 
# Load the packages
library(nycflights13)
library(tidyverse)
```

**a. Generate a table (which can just be a nicely printed tibble) reporting the mean and median departure delay per airport. Generate a second table (which again can be a nicely printed tibble) reporting the mean and median arrival delay per airport. Exclude any destination with under 10 flights. Do this exclusion through code, not manually.**

**Additionally,**

- Order both tables in descending mean delay.
- Both tables should use the airport names not the airport codes.
- Both tables should print all rows. 

```{r} 
# Generate table for mean and median departure delay per airport
flights %>%
    inner_join(airports, by = c("origin" = "faa")) %>%
    group_by(name) %>%
    summarise(mean_dep_delay = mean(dep_delay, na.rm = TRUE),median_dep_delay = median(dep_delay, na.rm = TRUE)) %>%
    arrange(desc(mean_dep_delay)) %>%
    print()
```

```{r} 
# Generate table for mean and median arrival delay per airport
flights %>%
    inner_join(airports, by = c("dest" = "faa")) %>%
    group_by(name) %>%
    filter(n() >= 10) %>%
    summarise(mean_arr_delay = mean(arr_delay, na.rm = TRUE),median_arr_delay = median(arr_delay, na.rm = TRUE)) %>%
    arrange(desc(mean_arr_delay)) %>%
    print()
```

**b. How many flights did the aircraft model with the fastest average speed take? Produce a tibble with 1 row, and entires for the model, average speed (in MPH) and number of flights.**

```{r}
# Generate table for the aircraft model with the fastest average speed
flights %>%
    inner_join(planes, by = c("tailnum")) %>%
    group_by(model) %>%
    summarise(avg_speed = mean(speed, na.rm = TRUE), num_flights = n()) %>%
    filter(num_flights > 0) %>%
    arrange(desc(avg_speed)) %>%
    head(1) %>%
    print()
```

## Problem 2 - `get_temp()` 

Use the `tidyverse` for this problem. In particular, use piping and `dplyr` as much as you are able. **Note:** Use of any deprecated functions will result in a point loss. 

Load the Chicago NNMAPS data we used in the visualization lectures. Write a function `get_temp()` that allows a user to request the average temperature for a given month. The arguments should be: 

- `month`: Month, either a numeric 1-12 or a string.
- `year`: A numeric year.
- `data`: The data set to obtain data from.
- `celsius`: Logically indicating whether the results should be in celsius. Default `FALSE`.
- `average_fn`: A function with which to compute the mean. Default is `mean`. 

The output should be a numeric vector of length 1. The code inside the function should, as with the rest of this problem, use `tidyverse`. Be sure to sanitize the input. 

Prove your code works by evaluating the following. Your code should produce the result, or a reasonable error message. 

```{r, eval = FALSE}
get_temp("Apr", 1999, data = nnmaps)
get_temp("Apr", 1999, data = nnmaps, celsius = TRUE)
get_temp(10, 1998, data = nnmaps, average_fn = median)
get_temp(13, 1998, data = nnmaps)
get_temp(2, 2005, data = nnmaps)
get_temp("November", 1999, data =nnmaps, celsius = TRUE,
        average_fn = function(x) {
            x %>% sort -> x
            x[2:(length(x) - 1)] %>% mean %>% return
        })
```

```{r} 
# Load the package and data 
library(tidyverse)
nnmaps <- read.csv("./chicago-nmmaps.csv")
head(nnmaps)
```

```{r}
# Define the function
get_temp <- function(month, 
                    year, 
                    data, 
                    celsius = FALSE, 
                    average_fn = mean) {
    # Sanitize the input
    months_abb <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
    months_name <- c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December")
    months_num <- c(1:12)

    if (month %in% months_abb) {
        month <- match(month, months_abb)
    } else if (month %in% months_name) {
        month <- match(month, months_name)
    }
    else if (month %in% months_num) {
        month <- as.numeric(month)
    } else {
        cat("Invalid month input", month, ". Should be an integer between 1 and 12 (e.g. 1) or a string of month name (e.g. January) or abbreviation (e.g. Jan). Not case sensitive.\n")
        return 
    }

    year <- as.integer(year)
    if (!is.integer(year) || year < 1997 || year > 2000) {
        cat("Invalid year input", year, ". Should be an integer between 1997 and 2000 (inclusive).\n")
        return 
    }
    if (!is.data.frame(data)) {
        cat("Invalid data input", data, ". Should be a data frame.\n")
        return 
    }
    if (!is.logical(celsius)) {
        cat("Input celsius input", celsius, ". Should be a logical value TRUE or FALSE.\n")
        return 
    }
    if (!is.function(average_fn)) {
        cat("Input average_fn input", average_fn, ". Should be a function.\n")
        return 
    }

    # To avoid name conflict, assign the input to temporary variables
    month_tmp <- month
    year_tmp <- year 
    
    # Compute the average temperature
    data %>%
        filter(month_numeric == month_tmp & year == year_tmp) %>%
        summarise(avg_temp = average_fn(temp)) %>%
        mutate(avg_temp = ifelse(celsius == TRUE, (avg_temp - 32) * 5 / 9, avg_temp)) %>%
        pull(avg_temp) %>%

    # Return a numeric vector of length 1
    return
}
```

```{r}
# Demo
get_temp("Apr", 1999, data = nnmaps)
get_temp("Apr", 1999, data = nnmaps, celsius = TRUE)
get_temp(10, 1998, data = nnmaps, average_fn = median)
get_temp(13, 1998, data = nnmaps)
get_temp(2, 2005, data = nnmaps)
get_temp("November", 1999, data =nnmaps, celsius = TRUE,
        average_fn = function(x) {
        x %>% sort -> x
        x[2:(length(x) - 1)] %>% mean %>% return
        })
```


## Problem 3 - SAS 

This problem should be done entirely within SAS. 

The `html` output can be found [here](https://github.com/cajole-fff/UMich-STATS506-FanZhang/blob/main/Problem%20Set%20%234/stats506_hw4p3_sas_result.html) and the `pdf` output can be found [here](https://github.com/cajole-fff/UMich-STATS506-FanZhang/blob/main/Problem%20Set%20%234/stats506_hw4p3_sas_result.pdf).


Access the RECS 2020 data and download a copy of the data. You may import the CSV or load the `sas7bdat` file directly. (This is **not** the 2009 version we used in lecture). You'll probably also need the "Variable and response cookbook" to identify the proper variables. Load or import data into SAS.

```{SAS, eval = FALSE}
/* Load the Data */
DATA RECS2020;
    SET '/home/u63648565/recs2020_public_v5.sas7bdat';
RUN;
```

**a. What state has the highest percentage of records? What percentage of all records correspond to Michigan? (Don't forget to account for the sampling weights!)**

```{SAS, eval = FALSE}
/* Calculate the percentage of records in each state, order by frequency */
PROC SURVEYFREQ DATA=RECS2020 ORDER=FREQ ;
    WEIGHT NWEIGHT;
    TABLES state_name;
    ODS OUTPUT OneWay=RECS2020FREQ;
RUN;

/* Print the state with the hightest percentage of records */
PROC PRINT DATA=RECS2020FREQ (OBS=1);
RUN;

/* Print the percentage of all records correspond to Michigan */
PROC PRINT DATA=RECS2020FREQ;
    WHERE state_name='Michigan';
RUN;
```

According to the result, the state with the highest percentage of records is *California*. 

**b. Generate a histogram of the total electricity cost in dollars, amongst those with a strictly positive cost.**

```{SAS, eval = FALSE}
/* Filter data with strictly positive cost */
DATA RECS2020_DOLLAREL_POS;
    SET RECS2020;
    IF dollarel > 0;
RUN;

/* Generate a histogram of the total electricity cost in dollars */
PROC UNIVARIATE DATA=RECS2020_DOLLAREL_POS;
    VAR dollarel;
    HISTOGRAM;
RUN;
```

**c. Generate a histogram of the log of the total electricity cost.**

```{SAS, eval = FALSE}
DATA RECS2020_DOLLAREL_POS;
    SET RECS2020_DOLLAREL_POS;
    IF dollarel > 0;
    log_dollarel = log(dollarel);
RUN;

/* Generate a histogram of the log of the total electricity cost */
PROC UNIVARIATE DATA=RECS2020_DOLLAREL_POS;
    VAR log_dollarel;
    HISTOGRAM;
RUN;
```

**d. Fit a linear regression model predicting the log of the total electricity cost based upon the number of rooms in the house and whether or not the house has a garage. (Don't forget weights.)**

```{SAS, eval = FALSE}
/* Fit a linear regression model predicting the log of the total electricity cost based upon the number of rooms in the house and whether or not the house has a garage */
PROC SURVEYREG DATA=RECS2020_DOLLAREL_POS;
    WEIGHT NWEIGHT;
    MODEL log_dollarel = totrooms prkgplc1;
    OUTPUT out=RECS2020_REG_RESULT p=predicted_log_dollarel;
RUN;
```

**e. Use that model to generate predicted values and create a scatterplot of predicted total electricity cost vs actual total electricity cost (not on the log scale).**

```{SAS, eval = FALSE}
/* Generate predicted values of dollarel */
DATA RECS2020_REG_RESULT;
    SET RECS2020_REG_RESULT;
    predicted_dollarel = exp(predicted_log_dollarel);
RUN;

PROC SGPLOT DATA=RECS2020_REG_RESULT;
    SCATTER x=dollarel y=predicted_dollarel;
RUN;
```

## Problem 4 - Multiple tools

It is not uncommon during an analysis to use multiple statistical tools as each has their own pros and cons. The problem is based on an actual analysis I've done, with a different data set. The data was originally stored in a large SAS database, but the researcher was most familiar with Stata so I carried out the analysis there. During the course of the project, there was a particular analysis that Stata could not do, so I switched over to R. We're going to mimic this workflow here.

We'll use the Survey of Household Economics and Decisionmaking from the Federal Reserve. The data and Codebook documentation can be found at [https://www.federalreserve.gov/consumerscommunities/shed_data.htm](https://www.federalreserve.gov/consumerscommunities/shed_data.htm). Use the 2022 version. 

The researcher's interest is in whether long-term concerns about climate change impact current day concerns about financial stability. To address this, the particular research question of interest is whether **the respondent's family is better off, or worse off financially compared to 12 month's ago** can be predicted by **thinking that the chance of experiencing a natural disaster or severe weather event will be higher, lower or about the same in 5 years**. We also want to control for 

- How they rate the economic conditions today in the country.
- Whether they own (with or without a mortgage) or rent or neither their home.
- Education (use the 4-category version)
- Race (use the 5-category version)

We're going to pretend the raw data is extremely large that wee need to extract the subset of the data we're going to use before we can open it in Stata or R. 

Additionally, the data comes from a complex survey design, so we need to account for that in the analysis. 

**a. Take a look at the Codebook. For very minor extra credit, how was the Codebook generated? (No loss of points if you skip this.**